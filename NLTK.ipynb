{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLTK.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2bgDLxGF7QC",
        "colab_type": "code",
        "outputId": "b6fa5781-ecdc-4f69-fe9a-2e1d7784f5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 840
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeMkwcbvHgTp",
        "colab_type": "code",
        "outputId": "85385f25-55fd-4f8c-b404-f887bce77987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
        "#using the sent_tokenize method we split the document or data into sentences\n",
        "print(sent_tokenize(data))\n",
        "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['All work and no play makes jack dull boy.', 'All work and no play makes jack a dull boy.']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['All work and no play makes jack dull boy.',\n",
              " 'All work and no play makes jack a dull boy.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pc9knkCIR2B",
        "colab_type": "code",
        "outputId": "01087371-85a0-4ab1-f677-b21961e3e5f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "#simlary using word_tokenize method the we split the sentences into words \n",
        "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
        "print(word_tokenize(data))\n",
        "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', ',', 'all', 'work', 'and', 'no', 'play']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['All',\n",
              " 'work',\n",
              " 'and',\n",
              " 'no',\n",
              " 'play',\n",
              " 'makes',\n",
              " 'jack',\n",
              " 'a',\n",
              " 'dull',\n",
              " 'boy',\n",
              " ',',\n",
              " 'all',\n",
              " 'work',\n",
              " 'and',\n",
              " 'no',\n",
              " 'play']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Tr3GCS-LICE",
        "colab_type": "code",
        "outputId": "e65499a6-4378-4f66-b98a-d5a33105705a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "data = \"All work and no play makes jack a dull boy, all work and no play\"\n",
        "words = word_tokenize(data)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "#{'at', 'here', 'once', 'or', 'them', 'doing', 'own', 'in', 'haven', 'further', 'for', 'if', 'than', 'aren', 'where', 'be', 'being', 'll', 'what', 'while', 'through', 'nor', 'yours', 'only', 'm', 'not', 'against', 'will', 'an', 'did', 'she', 'were', 'but', 'isn', 'few', 'which', 'just', 'myself', 'more', 'd', 'and', 'themselves', 'to', 'needn', 're', 'yourself', 'such', 'your', 'it', 'a', 'off', 'been', 'her', 'now', 'the', 'had', 'we', 'down', 'him', 'above', 'mightn', 'ma', 'on', 'no', 'ain', 'hasn', 'ours', 'both', 's', 'you', 'am', 'weren', 'don', 'shouldn', 'our', 'me', 'does', 'has', 'of', 'doesn', 't', 'before', 'theirs', 'each', 'over', 'out', 'most', 'yourselves', 'shan', 'because', 'have', 'into', 'during', 'too', 'itself', 'very', 'didn', 'this', 'himself', 'same', 'up', 'any', 'when', 'do', 'hadn', 'i', 'are', 'whom', 'other', 'again', 'with', 'won', 'couldn', 'these', 'his', 'why', 'then', 'o', 'hers', 'my', 'their', 'from', 'how', 'is', 'ourselves', 'mustn', 'its', 'should', 'he', 'as', 'wouldn', 'was', 'there', 'herself', 'they', 'y', 'wasn', 'that', 'under', 'so', 'some', 've', 'between', 'by', 'those', 'having', 'all', 'below', 'can', 'after', 'who', 'about', 'until'}"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'do', 'where', 'she', 'while', 'above', \"didn't\", \"isn't\", 'who', 'why', 'shan', 'were', 'below', 'its', 'this', \"couldn't\", 'll', 'an', 'no', \"hasn't\", 'that', 'having', 'been', 'y', \"doesn't\", 'because', 'ma', 'don', 'their', 'doesn', 'ain', \"you'd\", 'against', 'these', 'them', 'mightn', 'isn', 'being', 'by', 'there', 'it', \"should've\", 'own', 'with', 'most', 'in', 'are', 'here', \"won't\", 'once', 'further', 'a', 'when', 'had', 'needn', 'yourself', 'each', 'between', 'all', 'is', \"shouldn't\", \"wasn't\", 'you', 'does', 'me', 'other', 'so', 'shouldn', \"haven't\", 'mustn', 'has', 'the', 'what', \"that'll\", \"needn't\", 'about', 'yourselves', 'but', 'if', 'her', 'up', 'd', 'not', 'herself', 'then', 'wasn', 'themselves', 'before', \"wouldn't\", 'i', 'o', 'did', 'on', 'have', 'to', 'm', 'haven', 'into', \"you're\", 'weren', 'through', \"hadn't\", 'both', 'or', 't', 'himself', 'of', 'for', 'myself', 'hasn', 's', 've', 'off', 'didn', 'itself', 'those', 'him', 'at', 'doing', 'some', \"mustn't\", 'now', 'same', 'yours', \"you've\", 'until', 'very', 'just', 'too', 're', 'under', 'wouldn', \"it's\", \"don't\", \"you'll\", 'nor', 'hers', 'only', 'his', \"shan't\", 'again', 'can', 'from', \"aren't\", 'over', 'which', 'be', 'during', 'ours', 'out', 'down', 'such', 'won', 'whom', 'few', 'your', 'after', 'they', 'was', 'couldn', 'we', 'any', 'should', 'than', \"weren't\", 'how', 'am', 'hadn', 'our', 'ourselves', 'theirs', 'as', 'more', 'aren', \"she's\", 'and', \"mightn't\", 'will', 'my', 'he'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0dEkm9eL8lz",
        "colab_type": "code",
        "outputId": "d13942a9-dd56-4a34-d524-a87a7f48fb07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "filtered_word = []\n",
        "\n",
        "for wrd in words:\n",
        "    if wrd not in stop_words:\n",
        "        filtered_word.append(wrd)\n",
        "        \n",
        "print(filtered_word)\n",
        "#['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', ',', 'work', 'play']"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', ',', 'work', 'play']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNMLoav1P5tM",
        "colab_type": "text"
      },
      "source": [
        "NLTK - stemming\n",
        "A word stem is part of a word. It is sort of a normalization idea, but linguistic. For example, the stem of the word waiting is wait.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07sisECjP8LX",
        "colab_type": "code",
        "outputId": "66907b49-e191-4a20-ff98-8c1872b75a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "words = [\"game\",\"gaming\",\"gamed\",\"games\"]\n",
        "ps = PorterStemmer()\n",
        "\n",
        "for word in words:\n",
        "    print(ps.stem(word)) "
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "game\n",
            "game\n",
            "game\n",
            "game\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9leVrylRsQM",
        "colab_type": "text"
      },
      "source": [
        "NLTK - speech of tagging (POS)\n",
        "Given a sentence or paragraph, it can label words such as verbs, nouns and so on"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gbknH8rR6IT",
        "colab_type": "code",
        "outputId": "9a99d45e-6ba3-43fe-e36e-cada783051bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
        "sentences = nltk.sent_tokenize(document)   \n",
        "for sent in sentences:\n",
        "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Whether', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('new', 'JJ'), ('to', 'TO'), ('programming', 'VBG'), ('or', 'CC'), ('an', 'DT'), ('experienced', 'JJ'), ('developer', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('easy', 'JJ'), ('to', 'TO'), ('learn', 'VB'), ('and', 'CC'), ('use', 'VB'), ('Python', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwYVPPT8SWPT",
        "colab_type": "text"
      },
      "source": [
        "We can filter this data based on the type of word, especially if we are looking for nouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZOGNE2dSXHQ",
        "colab_type": "code",
        "outputId": "63d1e870-b0af-4f18-c4f0-32a4289320a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "document = 'Two military helicopters and six boats of the National Disaster Response Force were sent in to rescue as many as 900 people stranded on board a passenger train about 60 km out of Mumbai on Saturday after heavy rain paralysed the city and its surrounding areas.'\n",
        "sentences = nltk.sent_tokenize(document)   \n",
        "\n",
        "data = []\n",
        "for sent in sentences:\n",
        "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
        "\n",
        "print(data)\n",
        "\n",
        "for word in data: \n",
        "    if word[1] in ['NN', 'NNP', 'NNS']: \n",
        "        print(word)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Two', 'CD'), ('military', 'JJ'), ('helicopters', 'NNS'), ('and', 'CC'), ('six', 'CD'), ('boats', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('National', 'NNP'), ('Disaster', 'NNP'), ('Response', 'NNP'), ('Force', 'NNP'), ('were', 'VBD'), ('sent', 'VBN'), ('in', 'IN'), ('to', 'TO'), ('rescue', 'VB'), ('as', 'RB'), ('many', 'JJ'), ('as', 'IN'), ('900', 'CD'), ('people', 'NNS'), ('stranded', 'VBD'), ('on', 'IN'), ('board', 'NN'), ('a', 'DT'), ('passenger', 'NN'), ('train', 'NN'), ('about', 'IN'), ('60', 'CD'), ('km', 'NNS'), ('out', 'IN'), ('of', 'IN'), ('Mumbai', 'NNP'), ('on', 'IN'), ('Saturday', 'NNP'), ('after', 'IN'), ('heavy', 'JJ'), ('rain', 'NN'), ('paralysed', 'VBD'), ('the', 'DT'), ('city', 'NN'), ('and', 'CC'), ('its', 'PRP$'), ('surrounding', 'VBG'), ('areas', 'NNS'), ('.', '.')]\n",
            "('helicopters', 'NNS')\n",
            "('boats', 'NNS')\n",
            "('National', 'NNP')\n",
            "('Disaster', 'NNP')\n",
            "('Response', 'NNP')\n",
            "('Force', 'NNP')\n",
            "('people', 'NNS')\n",
            "('board', 'NN')\n",
            "('passenger', 'NN')\n",
            "('train', 'NN')\n",
            "('km', 'NNS')\n",
            "('Mumbai', 'NNP')\n",
            "('Saturday', 'NNP')\n",
            "('rain', 'NN')\n",
            "('city', 'NN')\n",
            "('areas', 'NNS')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mGxJ6eMXJtd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}